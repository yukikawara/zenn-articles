---
title: "Unlimiformer: Long-Range Transformers with Unlimited Length Input"
emoji: "📑"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["Transformer", "NLP"]
published: false
publication_name: "fusic"
---

こんにちは、初めましての方は初めまして。株式会社 [Fusic](https://fusic.co.jp/) の瓦です。GW は一日 14 時間くらい寝てしまい、気付いたら終わっていました。

この記事では *[Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/abs/2305.01625)* という論文の解説を行います。この論文では、モデルの受け取れる以上の長さの文は入力できない（後ろを切るなどして入力上限以内に抑える必要がある）という問題に対する解決を図っています。

# 概要

# 手法

# 実験
## データセット
実験に用いたデータセットは、`GovReport`, `SummScreen`, `BookSum` で、すべて長文の処理が必要なデータセットになっています。

# まとめ

最後に宣伝になりますが、機械学習でビジネスの成長を加速するために、[Fusic](https://fusic.co.jp/)の機械学習チームがお手伝いたします。機械学習のPoCから運用まで、すべての場面でサポートした実績があります。もし、困っている方がいましたら、ぜひ[Fusic](https://fusic.co.jp/)にご相談ください。[お問い合わせ](https://fusic.co.jp/contact/)からでも気軽にご連絡いただけます。また[TwitterのDM](https://twitter.com/kawara_fusic)からでも大歓迎です！
