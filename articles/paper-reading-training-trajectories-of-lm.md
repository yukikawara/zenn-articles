---
title: "Training Trajectories of Language Models Across Scales を読んだ"
emoji: "📑"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["LLM", "NLP", "ML"]
published: true
publication_name: fusic
---

こんにちは、初めましての方は初めまして。株式会社 Fusic の瓦です。ACL 2023 に受理された論文のリストが公開されましたね^[ [https://2023.aclweb.org/program/accepted/](https://2023.aclweb.org/program/accepted/)]。この記事ではその中から *[Training Trajectories of Language Models Across Scales](https://arxiv.org/abs/2212.09803)* という論文を選んで紹介したいと思います。

この論文では様々な大きさのモデルを対象に、訓練時の挙動の分析を行っています。推論時における分析はよく見かけますが、訓練時に何が起きているかを分析したものは珍しい気がします。解析に使用したコードは [Github](https://github.com/xiamengzhou/training_trajectory_analysis) にて公開されています。また、断りがない限りこの記事で使用している図は論文のものを使用しています。

## はじめに
自然言語処理において、言語モデルは非常に重要な役割を果たしており、テキストの自動生成や翻訳、質問応答などの様々なタスクで活用されています。しかし、言語モデルの性能は、そのサイズや学習に使用したデータ、トレーニング方法によって大きく異なることが知られています。本論文では、異なるサイズの言語モデルについて、トレーニング中の言語モデルの挙動や、ダウンストリームタスクでのパフォーマンスについて検討しています。本論文の結果は、言語モデルの設計やトレーニング方法に関する今後の研究に大きな示唆を与えるものとなっています。

## 実験設定
この論文ではモデルの大きさ以外の条件を揃えるために、Meta 社が公開している Open Pretrained Transformer (OPT) を使用しています。使用したモデルとそのチェックポイントは下の図のようになっています。

![](/images/paper-reading-training-trajectories-of-llm/opt-checkpoints.png)

モデルの評価としては検証データにおけるパープレキシティ (PPL) を使用しています。パープレキシティとは、ざっくり言うと次に来ると予測した単語の数になります。この値が例えば 100 だと、モデルが次に出す単語を 100 個の選択肢まで絞り込めているということになります。この値が小さければ小さいほど、モデルが次に出す単語を絞り込めている = 正しい文を生成しやすくなっているということになります。パープレキシティについて詳しく知りたい人は mm_0824 さんの[【入門者向け】Perplexityを直観的に理解する](https://data-analytics.fun/2022/01/15/understanding-perplexity/)を参照していただければと思います。

## 実験結果
この論文では各モデルに対して

- Next-Token Prediction (単語レベルでの性能)
- Sequence-Level Generation (文レベルでの性能)
- Downstream Tasks (ダウンストリームでの性能)

の分析を行っています。以下ではそれぞれに対して詳細を見てみたいと思います。

### Next-Token Prediction
次の単語予測では、各チェックポイントにおけるそれぞれの単語を以下の三つのカテゴリに分けています。

- PPL が下がっている単語 (学習中の単語)
- PPL が上がっている単語 (忘れている単語)
- 上記二つ以外の単語 (既に学習が終わり、忘れてもいない単語)

これら三つのカテゴリに分けて分析を行うことで、モデルがどのような学習を行っているのかを理解しようとしているのだと思います。

![](/images/paper-reading-training-trajectories-of-llm/fig2.png)

まず全体的な傾向として、**学習が進むにつれて学習済みの単語の割合は増加**しており、**まだ学習中の単語や忘れていっている単語の割合は減少**しています。どの大きさのモデルでも、このデータセットおよびこのステップ数では全ての単語の学習が終わっているということはなさそうです。

次にあるモデルでの各カテゴリの単語集合が、他のモデルではどのような振る舞いをするのかを分析しています。まず学習済みの単語集合に対しての分析です。訓練データに含まれている学習済みの単語のうち 10% を抽出して、それらの単語の他のモデルにおける PPL をプロットしています。

![](/images/paper-reading-training-trajectories-of-llm/fig3.png)

真ん中の図は 1.3B のモデルでの学習済み単語集合に対して各モデルで PPL を計測したものになっています。これを見ると、**小さなモデルで学習済みの単語は大きなモデルでも同様に学習が進んでおり、PPL が収束している**ことが分かります。データセットが同じだからなのでしょうが、モデルの大きさが違っても同じような単語集合を学習しているというのは面白いです。
右側の図は、175B のモデルでの学習済み単語集合に対して各モデルで PPL を計測したものになっています。こちらでは、小さなモデルではまだ PPL が下がっていることが分かります (被っているので分かりづらいですが…) このことから、**大きなモデルで学習している単語集合は小さなモデルで学習した単語集合を含むようなものになっている**ということが分かります。より大きなモデルの方が同じステップ数でより速く学習が出来ているということでしょうか。

![](/images/paper-reading-training-trajectories-of-llm/fig4.png)

次に PPL が上がっている単語集合に対して各モデルで PPL を測ったものになります。こちらでは少し面白い現象が起こっています。真ん中の図を見ると、小さなモデルで PPL が上がっている単語集合は大きなモデルでも PPL が上がっています。しかし、ある程度学習させ続けるとまた PPL が下がり始めるという現象 (double descent) が起きています。この結果から、「小さなモデルでも大きなモデルでも、まずある程度の小さな単語集合に対して学習が進む。その後、大きなモデルは残りの単語に対しても学習が進む。」と結論付けています。データセットが同じなのでまずは同じような単語集合を学習しやすいのではないかと思うのですが、その後大きなモデルはさらに別の単語も学習していくというのは面白いと思います。

### Sequence-Level Generation
次に文の生成に対する能力の分析になります。学習済みのモデルではデータセットの文、つまり人が書いた文をより出力しやすい傾向にあるはずです。しかしそうではないような文に対してはどのような傾向にあるのでしょうか。著者らはモデルが大きいと不自然な文に対する PPL は上がるはずだと予想しており、間違った文に対しての分析を行っています。

![](/images/paper-reading-training-trajectories-of-llm/fig5.png)

上の左の図は、文中のいくつかの単語をランダムな別の単語に置き換えたもので PPL を測ったものです。置き換える割合が増えていくともちろん PPL が増えていっています。しかし、100% ランダムな単語に置き換えてもモデルを大きくすると PPL が下がっていっていることは面白い現象です。この現象について、著者らは文脈から予測する能力が大きなモデルの方が高いからと結論付けていますが、「ランダムなのにパターンがあるのか…？」と個人的に疑問に思っています。ある単語と置き換え先の単語が決まっているなら分からないでもないですが、正直ここはあまり理解が出来ていません。

右側の図では、多肢選択問題に対する PPL を測っています。著者らは大きなモデルでは間違った選択肢に対して確率が低くなるはずだと予想していますが、ここでもモデルが大きくなるにつれて PPL が下がっています。個人的には、間違った選択肢もある程度問題文と関連した内容を含んでいると思うので、そのような文に対して大きなモデルの方が間違いを捉えやすいということがあるのではないかと思います。

### Downstream Tasks
さらに、ダウンストリームでのタスクに対して、few-shot での in-context learning による評価を行っています。ダウンストリームでの評価とモデルのそれぞれの評価値のグラフを下に載せます。

![](/images/paper-reading-training-trajectories-of-llm/fig8.png)

これを見ると、**PPL と精度の間に強い相関があり、かつモデルの大きさではなく PPL で精度が決まる**ということも分かります。FLOPs と精度のグラフを見ると、必ずしも大きなモデルがいつでもいい性能を示すという訳ではないようです。FLOPs が小さい時には小さいモデルの方が性能が高くなっています。（僕の FLOPs の認識が間違っているかもしれませんが）ちゃんと学習した小さなモデルとあまり学習していない大きなモデルでは、学習した小さなモデルの方が性能がいいということだと思います。またトークン数や学習ステップ数の違いによる精度の違いもプロットしています。こちらもこれまでに知られているようなものが実際に確認できていると感じですね。

## まとめ
この記事では LLM についてどういう学習を行っているかを分析した論文の解説を行いました。学習済みのモデルでの分析は多くありますが、どのような学習を行っているのかを分析した論文は珍しいと思います。個人的には大きなモデルで PPL の上がった単語が訓練を続けることで PPL がまた下がっていくという現象が、学習しやすい単語だけとりあえず先に学習してその後他の単語を学習していくという人間らしい？プロセスで大変興味深いです。

最後に宣伝になりますが、機械学習でビジネスの成長を加速するために、[Fusic](https://fusic.co.jp/)の機械学習チームがお手伝いたします。機械学習のPoCから運用まで、すべての場面でサポートした実績があります。もし、困っている方がいましたら、ぜひ[Fusic](https://fusic.co.jp/)にご相談ください。[お問い合わせ](https://fusic.co.jp/contact/)からでも気軽にご連絡いただけます。また[TwitterのDM](https://twitter.com/kawara_fusic)からでも大歓迎です！